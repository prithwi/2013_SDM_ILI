% mainfile: ../ltexpprt.tex

\subsection{Reference Data.}
In this paper, we focus on 15 Latin
American countries viz. Argentina, Bolivia, Costa Rica, Colombia, Chile,
Ecuador,  El Salvador, Guatemala, French Guiana, Honduras, Mexico, Nicaragua,
Paraguay, Panama and Peru. We collected weekly ILI counts
from the official Pan American Health Organization (PAHO)
website~\cite{PAHO:2013}, {\bf every day} from January 2013 to August 2013. The
estimates downloaded every day for each country contain data from January 2010 to the latest
available week on the day of collection. 
%The downloaded data may contain some weeks with no ILI estimates and its next 
%pre-processed to fill the missing using linear interpolation. 
This dataset is stored in a database we refer to as
the {\it Temporal Data Repository} (TDR).  The TDR is also timestamped so that for
any given day, we can readily retrieve the ILI case
counts that were download on that day. This is important as historic data may be updated by
PAHO even a number of weeks after the first update.  For the purpose of experimental validation
we used the data for the period Jan 2010 to December 2012 as the static
training set. We considered Wednesdays of the weeks as a reference day within a week.  For
each Wednesday from Jan 2013 to July 2013, we used the latest available PAHO
data in TDR for that day and predicted 3 weeks from the last available week for
which the PAHO data was available. These predictions are next evaluated against
the final ILI case count as downloaded on September 1, 2013 and we report the
performance of our algorithms in Section~\ref{sec:results}. 

\subsection{Evaluation criteria.}
We evaluate the prediction accuracy of the
different algorithms using a modified version of percentage relative error:

\begin{equation} 
    \label{eq:def:accuracy} 
    \mathcal{A} = \frac{4}{N_p}\sum \limits_{t=t_s}^{t_e}\frac{|P_t -\hat{P}_t| }{max(P_t, \hat{P}_t, 10)}
\end{equation} 
where $t_s$ and $t_e$ indicate the starting and the ending
time point for which predictions were generated.  $N_p$ indicates the number of
time points over the same time period (i.e. $N_p = t_e - t_s + 1$). Note that
the measure is scaled to have values in $[0,4]$ and the denominator is
designed to not over-penalize small deviations from the true ILI case count (e.g., when
the true case count is 0 and the predicted count is 1).
It is to be noted that the accuracy metric so defined is
non-convex and is in general multi-modal. 

%The accuracy criterion was designed according the following criteria:
%\begin{itemize} 
%    \item 
%    The accuracy estimates should reflect the percentage
%    deviation in prediction rather than absolute deviation. 
%     \prithwish{should we justify this?} 
%\item The accuracy estimates must be non-negative.  \item If we
%don't consider the max operator in the denominator, even ``good'' predictions
%such as 1 for an actual count of 0 will be assigned an accuracy score of 0. As
%such we chose a threshold (here 10) for which very small deviations are not
%over-penalized. The threshold was chosen by manually inspecting the span of ILI
%case counts (which were found to lie between 0 and 2000) and subsequent
%feedback from subject experts about the ``interesting'' range of predictions.
%\end{itemize} 


\subsection{Surrogate data sources.} Before
describing our data sources in detail, we describe our overall methodology
for organizing a flu-related disctionary (for tracking in multiple media such
as news, tweets, and search queries).

\subsubsection{\label{sec:keyword} Dictionary creation.} \input{tex/keywords}

We next describe the different data sources.

\subsubsection{Google Flu Trends ($\mathcal{F}$):}
Google Flu Trends~\cite{GFT:2013} (GFT) is a tool based
on~\cite{ginsberg2008detecting} and provided by Google.org which gives weekly
and up-to-date ILI case count estimates using search query volumes. 
Of the countries under
consideration, GFT provides weekly estimates for only 6 of them viz.  Argentina,
Bolivia, Chile, Mexico, Peru and Paraguay. 
These estimates are typically at
a different scale than the ILI case counts provided by PAHO and therefore need
to be scaled accordingly.  We collected this data weekly on Monday from Jan
2013 to Aug 2013. (The data downloaded on a particular day contains the entire
time-series from 2004 to the corresponding week.)
 
\subsubsection{Google Search Trends ($\mathcal{S}$):} Google Search Trends~\cite{GST:2013} is
another tool provided by Google. Using this tool we can download an estimate of
search query volume as a percentage over its own temporal history, filtered
geographically. We download
the search query volume time series for the 114 keywords described earlier
and convert the percentage measures to
absolute values using a static dataset we downloaded on Oct 2012 when Google
Search Trends used to provide absolute query volumes. 

\subsubsection{Twitter ($\mathcal{T}$):} 
Twitter data was collected from Datasift.com
and geotagged using an in-house geocoder. We lemmatized the tweet
contents and used language detection and POS tagging to help differentiate relevant
from irrelevant uses of our keywords 
(e.g., the Spanish word
{\it gripe}, meaning flu, is part of our flu keyword list as opposed to the undesired
and unrelated English word `gripe'). The resulting analysis yields a weekly occurrence
count of our dictionary in tweets.
%Also using the lemmatized versions we
%can match between different parts-of-speech variations of the same word. Using
%the lemma sized equivalents of our keyword dictionary we next parsed the
%enriched Tweets for possible matches with flu related keywords and constructed
%a time series of flu keyword weekly occurrence count. For the country under
%consideration, we denote this time series by 
%$\mathcal{T} = \langle \mathcal{T}_1, \mathcal{T}_2, \dots, \mathcal{T}_{T1} \rangle$.
%
%We started collecting flu filtered Twitter Data from November 2012 and everyday
%we download the corresponding Twitter time-series for each country. As shown in
%Figure~\ref{fig:ili_data_pipeline}, every week we collect 10GB of raw Twitter
%Data which are next enriched to 20GB of structured data. From this dataset, we
%parse flu-related Tweets resulting in around 7GB of data. This data is finally
%abstracted into a multivariate time series over lemmatized versions of the 114
%flu related keywords and added to the TDR. 

\subsubsection{HealthMap ($\mathcal{H}$):} 
Similar to Twitter data, we also collect flu-related
news stories using HealthMap~\cite{HM:2013}, an online global disease alert
system capturing outbreak data from over 50,000 electronic sources [including
expert-curated accounts, such as ProMED-Mail~\cite{chase1996promed}, news media
and official reports from local, national and global public health
organizations.] Using this service we receive flu-related news as a daily feed
which is similarly enriched and filtered to obtain
a multivariate time series over lemmatized version of the keywords. 
While Twitter is more suitable to ascertain general public response, the
HealthMap data provides more detailed information but may capture the trends
at a slower rate. Thus each of these sources offers utility in
capturing different surrogate signals: Twitter offers leading but noisy
indicators whereas HealthMap provides a slightly delayed but more reliable
indicator.

\subsubsection{OpenTable ($\mathcal{O}$):}
We also use data on trends of restaurant table reservations, initially 
studied in~\cite{elaine2013opentable} to be a potential early indicator for
outbreak surveillance, as another surrogate for ILI detection.
This novel data stream is based on the
postulate that a higher than average number of restaurants with table
availabilities in a region can serve as an indicator of an event of interest,
such as increase in flu cases. Table availability was monitored using OpenTable~\cite{Opentable:2013}, 
an online restaurant reservation site with 28,000 restaurants at the time
of this writing. Daily searches were performed starting from September 2012 for
a table for two persons at lunch and dinner; between 12:30-3pm, and between
6-10:30pm. Data was collected for Mexico by city (Cancun, Mexico City, Puebla,
Monterrey, and Guadalajara) and for the entire country. The daily proportion
(proportion used due to changes in the number of restaurants in the system) of
restaurants with available tables was aggregated as a weekly time-series.

%--Since we monitored ten regions at twenty search times, this resulted in 200
%distinct time-series curves.
\begin{figure} 
  \includegraphics[width=3.33in]{fig/humidity_centers.pdf}
  \caption{\label{fig:surveillance} PAHO surveillance centers for the 15 Latin
  American countries.}
\end{figure}

\subsubsection{Weather ($\mathcal{W}$):}
All of the previously described data sources can
be termed as non-physical indicators. These data sources can work as indirect
indicators about the state of the population with respect to flu by exposing
different population characteristics. At the same time, meteorological data can
be considered a more direct and physical driver of influenza transmission~\cite{flu_humidity_physical}. It 
has been shown in~\cite{Shaman_orig_humidity_link, Shaman_humidity_USA, ref9}
that absolute humidity can be directly used to predict the onset of influenza
epidemics. Here, we collected several other meteorological factors such as
temperature and rainfall in addition to humidity from the Global Data
Assimilation System (GDAS) which is a series of weather models run by National
Weather Service's National Centers for Environmental Prediction that provides
detailed meteorological data in near real-time across the globe.  We accessed
the data via an archive hosted by NASA (\url{http://ladsweb.nascom.nasa.gov/})~\cite{HD:2013}, 
and provided in GRIB format. By specifying the resolution and
the geographical boundaries we can collect the data for the countries under
consideration to a resolution of 1 degrees lat/long interval. However, looking
at all the lat/long for a country can often lead to noisy data. As such we
filtered the downloaded data and parsed the meteorological data only around the
PAHO surveillance centers as shown in Fig.~\ref{fig:surveillance}. We also
aggregate this data using weekly averages and thus obtain a resultant time series
for each country. We collected this data weekly from Jan 2013 to August 2013. 

\subsubsection{Temporal Data Repository}
All of the described data sources were
added to a temporal database, ``Temporal Data Repository'' (TDR). Some of the
data sources such as the Weather Data, Twitter Data, HealthMap Data and
OpenTable data once downloaded doesn't change. Also at the time of download
these datasets are updated till the corresponding day.  Thus to extract the
data available at a certain time point for these data sets we can simply
consider the corresponding truncated time series from the latest data. However,
Statistics like Google Search Trends and the PAHO ILI case counts exhibit
variability in past data. In these cases, the actual time series for these datasets are
stored in TDR with the corresponding timestamps. Thus given any time point,
using TDR we can quickly reference the data available at that point of time and
run our algorithm to simulate a prospective analysis scenario.  The entire data
collection process is depicted pictorially in
Figure~\ref{fig:ili_data_pipeline}.



